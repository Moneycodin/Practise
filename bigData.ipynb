{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6993401",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Explain the core components of the Hadoop ecosystem and their respective roles in processing and\n",
    "storing big data. Provide a brief overview of HDFS, MapReduce, and YARN.\n",
    "\n",
    "\n",
    "\n",
    "Core Components of the Hadoop Ecosystem\n",
    "HDFS (Hadoop Distributed File System):\n",
    "\n",
    "Role: Stores large datasets across a distributed cluster.\n",
    "Components:\n",
    "NameNode: Manages metadata and file system namespace.\n",
    "DataNode: Stores actual data blocks and handles read/write requests.\n",
    "Function: Provides reliable, scalable, and fault-tolerant storage.\n",
    "MapReduce:\n",
    "\n",
    "Role: Processes large datasets in a distributed manner.\n",
    "Phases:\n",
    "Map: Splits and processes input data to produce key-value pairs.\n",
    "Reduce: Aggregates and processes these key-value pairs to produce final results.\n",
    "Function: Enables parallel processing of data across the cluster.\n",
    "YARN (Yet Another Resource Negotiator):\n",
    "\n",
    "Role: Manages cluster resources and schedules applications.\n",
    "Components:\n",
    "ResourceManager: Allocates resources and manages job scheduling.\n",
    "NodeManager: Manages resources and execution on individual nodes.\n",
    "Function: Provides resource management and job scheduling, allowing multiple applications to run concurrently.\n",
    "These components together support the scalable storage, processing, and management of big data in a distributed environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036a95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de536cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Discuss the Hadoop Distributed File System (HDFS) in detail. Explain how it stores and manages data in a\n",
    "distributed environment. Describe the key concepts of HDFS, such as NameNode, DataNode, and blocks, and\n",
    "how they contribute to data reliability and fault tolerance.\n",
    "\n",
    "\n",
    "Hadoop Distributed File System (HDFS)\n",
    "**1. Data Storage:\n",
    "\n",
    "Blocks: Files are split into fixed-size blocks (e.g., 128 MB) and stored across multiple nodes.\n",
    "Replication: Each block is replicated (default is 3 times) to ensure reliability and fault tolerance.\n",
    "**2. Key Components:\n",
    "\n",
    "NameNode: Manages the metadata (file names, block locations) and namespace. It keeps track of where blocks are stored across the cluster but does not store the data itself.\n",
    "DataNode: Stores the actual data blocks and handles read/write requests from clients. Multiple DataNodes replicate blocks to ensure fault tolerance.\n",
    "**3. Fault Tolerance:\n",
    "\n",
    "Data Replication: Blocks are replicated across different DataNodes. If a DataNode fails, other replicas are used to retrieve data.\n",
    "Heartbeat and Block Report: DataNodes periodically send heartbeats and block reports to the NameNode to ensure they are functioning correctly.\n",
    "HDFS ensures data reliability and fault tolerance by replicating data across multiple nodes and managing metadata efficiently with the NameNode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Write a step-by-step explanation of how the MapReduce framework works. Use a real-world example to\n",
    "illustrate the Map and Reduce phases. Discuss the advantages and limitations of MapReduce for processing\n",
    "large datasets.\n",
    "\n",
    "MapReduce Framework Overview\n",
    "Input Data: Data is split into chunks (blocks) and distributed across the cluster.\n",
    "\n",
    "Map Phase:\n",
    "\n",
    "Function: Processes each chunk and generates intermediate key-value pairs.\n",
    "Example: Counting words in a document. Mapper emits pairs like (\"word\", 1).\n",
    "Shuffle and Sort:\n",
    "\n",
    "Function: Groups and sorts intermediate pairs by key.\n",
    "Reduce Phase:\n",
    "\n",
    "Function: Aggregates the grouped pairs to produce final results.\n",
    "Example: Summing counts for each word to get total word frequencies.\n",
    "Output: Results are written to the output location.\n",
    "\n",
    "Advantages\n",
    "Scalability: Processes large datasets across many nodes.\n",
    "Fault Tolerance: Handles node failures by reprocessing data.\n",
    "Limitations\n",
    "Complexity: Requires writing custom code for Map and Reduce functions.\n",
    "Performance: Disk I/O can be slow due to frequent read/write operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d800ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Explore the role of YARN in Hadoop. Explain how it manages cluster resources and schedules applications.\n",
    "Compare YARN with the earlier Hadoop 1.x architecture and highlight the benefits of YARN.\n",
    "\n",
    "Role of YARN in Hadoop\n",
    "YARN (Yet Another Resource Negotiator): Manages cluster resources and schedules applications in Hadoop.\n",
    "Functions:\n",
    "\n",
    "Resource Management: Allocates resources (CPU, memory) across different applications.\n",
    "Application Scheduling: Schedules and manages the execution of applications on the cluster.\n",
    "Comparison with Hadoop 1.x\n",
    "Hadoop 1.x Architecture: Had a single JobTracker for resource management and job scheduling, which led to scalability issues.\n",
    "\n",
    "YARN Benefits:\n",
    "\n",
    "Scalability: Separates resource management and job scheduling into ResourceManager and NodeManagers, allowing for better scalability.\n",
    "Resource Utilization: Allows multiple types of applications (e.g., MapReduce, Spark) to run concurrently and efficiently.\n",
    "Fault Tolerance: Enhances fault tolerance and resource allocation flexibility.\n",
    "YARN improves cluster resource management and application scheduling compared to the older Hadoop 1.x architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3376a5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Provide an overview of some popular components within the Hadoop ecosystem, such as HBase, Hive, Pig,\n",
    "and Spark. Describe the use cases and differences between these components. Choose one component and\n",
    "explain how it can be integrated into a Hadoop ecosystem for specific data processing tasks.\n",
    "\n",
    "Hadoop Ecosystem Components\n",
    "HBase: NoSQL database for real-time read/write access to large datasets. Useful for applications needing fast, random data access.\n",
    "\n",
    "Hive: SQL-like interface for querying large datasets on Hadoop. Ideal for data analysis and reporting.\n",
    "\n",
    "Pig: Scripting platform for creating MapReduce programs. Simplifies ETL tasks and data transformations.\n",
    "\n",
    "Spark: In-memory data processing engine. Supports real-time processing, batch processing, and machine learning.\n",
    "\n",
    "Integration Example: Spark\n",
    "Scenario: Real-time log data processing\n",
    "\n",
    "Data Ingestion: Use Spark Streaming to read data from sources like Kafka.\n",
    "Processing: Apply transformations and analyses with Spark’s RDDs or DataFrames.\n",
    "Storage: Save results to HDFS or HBase.\n",
    "Querying: Use Hive for SQL queries on processed data if stored in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c7a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Explain the key differences between Apache Spark and Hadoop MapReduce. How does Spark overcome\n",
    "some of the limitations of MapReduce for big data processing tasks?\n",
    "\n",
    "\n",
    "\n",
    "Key Differences between Apache Spark and Hadoop MapReduce\n",
    "Processing Model:\n",
    "\n",
    "MapReduce: Batch processing; processes data in a series of map and reduce steps.\n",
    "Spark: In-memory processing; uses Resilient Distributed Datasets (RDDs) for fast, iterative computations.\n",
    "Performance:\n",
    "\n",
    "MapReduce: Writes intermediate data to disk, leading to slower performance due to disk I/O.\n",
    "Spark: Keeps data in memory between operations, resulting in much faster processing.\n",
    "Ease of Use:\n",
    "\n",
    "MapReduce: Requires writing complex code in Java, with low-level APIs.\n",
    "Spark: Provides high-level APIs in Java, Scala, Python, and R, making it easier to use.\n",
    "Fault Tolerance:\n",
    "\n",
    "MapReduce: Relies on data replication and recomputation on failure.\n",
    "Spark: Uses lineage information to recompute lost data, offering more efficient fault tolerance.\n",
    "Data Processing:\n",
    "\n",
    "MapReduce: Best suited for batch processing.\n",
    "Spark: Supports batch, interactive, streaming, and machine learning workloads.\n",
    "How Spark Overcomes MapReduce Limitations:\n",
    "\n",
    "In-Memory Computation: Spark processes data in memory to avoid slow disk reads/writes.\n",
    "Unified Framework: Spark provides a unified framework for various data processing tasks beyond batch processing.\n",
    "High-Level APIs: Simplifies development with easier-to-use APIs and libraries.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791ff86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Write a Spark application in Scala or Python that reads a text file, counts the occurrences of each word,\n",
    "and returns the top 10 most frequent words. Explain the key components and steps involved in this\n",
    "application.\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load text file into RDD\n",
    "text_file = sc.textFile(\"path/to/textfile.txt\")\n",
    "\n",
    "# Split lines into words, map to (word, 1), and reduce by key to count occurrences\n",
    "word_counts = text_file.flatMap(lambda line: line.split()) \\\n",
    "                       .map(lambda word: (word, 1)) \\\n",
    "                       .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Get top 10 most frequent words\n",
    "top_10_words = word_counts.takeOrdered(10, key=lambda x: -x[1])\n",
    "\n",
    "# Print results\n",
    "for word, count in top_10_words:\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n",
    "2. Key Components and Steps\n",
    "\n",
    "SparkSession: Initializes the Spark application.\n",
    "textFile(): Reads the text file into an RDD.\n",
    "flatMap(): Splits each line into words.\n",
    "map(): Maps each word to a tuple (word, 1).\n",
    "reduceByKey(): Aggregates counts for each word.\n",
    "takeOrdered(): Retrieves the top 10 most frequent words.\n",
    "stop(): Ends the Spark session.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a6d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Using Spark RDDs (Resilient Distributed Datasets), perform the following tasks on a dataset of your\n",
    "choice:\n",
    "a. Filter the data to select only rows that meet specific criteria.\n",
    "\n",
    "filtered_rdd = rdd.filter(lambda row: row[criteria_column] > threshold)\n",
    "\n",
    "b. Map a transformation to modify a specific column in the dataset.\n",
    "\n",
    "mapped_rdd = filtered_rdd.map(lambda row: (row[0], row[1] * 2))\n",
    "\n",
    "c. Reduce the dataset to calculate a meaningful aggregation (e.g., sum, average).\n",
    "\n",
    "result = mapped_rdd.map(lambda row: row[1]).reduce(lambda a, b: a + b)  # Sum example\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85573a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Create a Spark DataFrame in Python or Scala by loading a dataset (e.g., CSV or JSON) and perform the\n",
    "following operations:\n",
    "a. Select specific columns from the DataFrame.\n",
    "b. Filter rows based on certain conditions.\n",
    "c. Group the data by a particular column and calculate aggregations (e.g., sum, average).\n",
    "d. Join two DataFrames based on a common key.\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkDataFrameOperations\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load dataset into DataFrame\n",
    "df = spark.read.csv(\"path/to/your/dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "#selecting specific column\n",
    "selected_df = df.select(\"column1\", \"column2\")\n",
    "\n",
    "#filter row based on conditions\n",
    "filtered_df = df.filter(df[\"column1\"] > 10)\n",
    "\n",
    "#Group Data and Calculate Aggregations\n",
    "aggregated_df = df.groupBy(\"group_column\").agg(\n",
    "    {\"numeric_column\": \"sum\", \"numeric_column\": \"avg\"}\n",
    ")\n",
    "\n",
    "Join Two DataFrames\n",
    "df2 = spark.read.csv(\"path/to/another/dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Perform join\n",
    "joined_df = df.join(df2, df[\"common_key\"] == df2[\"common_key\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01239f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Set up a Spark Streaming application to process real-time data from a source (e.g., Apache Kafka or a\n",
    "simulated data source). The application should:\n",
    "a. Ingest data in micro-batches.\n",
    "b. Apply a transformation to the streaming data (e.g., filtering, aggregation).\n",
    "c. Output the processed data to a sink (e.g., write to a file, a database, or display it).\n",
    "\n",
    "\n",
    "\n",
    "Spark Streaming Application Setup\n",
    "Requirements:\n",
    "\n",
    "Apache Spark and Spark Streaming\n",
    "Apache Kafka (for real-time data source)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkStreamingExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_bootstrap_servers = 'localhost:9092'\n",
    "kafka_topic = 'my_topic'\n",
    "\n",
    "# Create DataFrame from Kafka stream\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .load()\n",
    "\n",
    "# Apply transformation (e.g., filtering messages)\n",
    "transformed_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .filter(col(\"value\").contains(\"specific_keyword\"))\n",
    "\n",
    "# Output to a sink (e.g., write to console)\n",
    "query = transformed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Await termination\n",
    "query.awaitTermination()\n",
    "\n",
    "Explanation:\n",
    "\n",
    "a. Ingest Data: Reads data from Kafka in micro-batches.\n",
    "b. Transform Data: Filters messages containing a specific keyword.\n",
    "c. Output Data: Displays the processed data to the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4154b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dae0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dcea80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db26ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. Explain the fundamental concepts of Apache Kafka. What is it, and what problems does it aim to solve in\n",
    "the context of big data and real-time data processing?\n",
    "\n",
    "\n",
    "Apache Kafka is a distributed streaming platform that handles high-throughput, real-time data.\n",
    "\n",
    "Key Concepts:\n",
    "\n",
    "Producer: Sends data to Kafka.\n",
    "Topic: Categories where data is stored.\n",
    "Partition: Splits topics for parallel processing.\n",
    "Broker: Manages data storage and requests.\n",
    "Consumer: Reads and processes data.\n",
    "ZooKeeper: Coordinates and manages Kafka clusters.\n",
    "Problems Solved:\n",
    "\n",
    "High Throughput: Handles large volumes of data efficiently.\n",
    "Real-Time Processing: Supports low-latency data streaming.\n",
    "Fault Tolerance: Replicates data to prevent loss.\n",
    "Decoupling: Separates producers from consumers for flexibility.\n",
    "Stream Processing: Enables real-time data transformations.\n",
    "Data Integration: Facilitates data movement between systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1427f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. Describe the architecture of Kafka, including its key components such as Producers, Topics, Brokers,\n",
    "Consumers, and ZooKeeper. How do these components work together in a Kafka cluster to achieve data\n",
    "streaming?\n",
    "\n",
    "\n",
    "Kafka Architecture in Short\n",
    "Producers: Send data to Kafka topics.\n",
    "Topics: Logical channels where data is stored, divided into partitions.\n",
    "Brokers: Servers that store and manage data for topics, handling read and write requests.\n",
    "Consumers: Read and process data from topics.\n",
    "ZooKeeper: Coordinates Kafka brokers, manages metadata, and handles leader election for partitions.\n",
    "How They Work Together:\n",
    "\n",
    "Producers push messages to topics via brokers.\n",
    "Brokers store and replicate messages in partitions.\n",
    "Consumers fetch and process messages from topics.\n",
    "ZooKeeper ensures cluster coordination and fault tolerance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbf5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "13. Create a step-by-step guide on how to produce data to a Kafka topic using a programming language of\n",
    "your choice and then consume that data from the topic. Explain the role of Kafka producers and consumers\n",
    "in this process.\n",
    "\n",
    "Producing and Consuming Data in Kafka\n",
    "1. Setup\n",
    "Install Kafka: Follow the Kafka quick start guide.\n",
    "Install Python Client:\n",
    "bash\n",
    "Copy code\n",
    "pip install confluent_kafka\n",
    "2. Create a Kafka Topic\n",
    "Create Topic:\n",
    "bash\n",
    "Copy code\n",
    "kafka-topics.sh --create --topic my_topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n",
    "3. Produce Data\n",
    "Producer Script (producer.py):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    if err:\n",
    "        print(f\"Delivery failed: {err}\")\n",
    "    else:\n",
    "        print(f\"Delivered to {msg.topic()} [{msg.partition()}]\")\n",
    "\n",
    "producer = Producer({'bootstrap.servers': 'localhost:9092'})\n",
    "for i in range(5):\n",
    "    producer.produce('my_topic', f\"Message {i}\".encode('utf-8'), callback=delivery_report)\n",
    "    producer.poll(1)\n",
    "producer.flush()\n",
    "Run Producer:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "python producer.py\n",
    "4. Consume Data\n",
    "Consumer Script (consumer.py):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from confluent_kafka import Consumer, KafkaException\n",
    "\n",
    "consumer = Consumer({\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'my_consumer_group',\n",
    "    'auto.offset.reset': 'earliest'\n",
    "})\n",
    "consumer.subscribe(['my_topic'])\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg and not msg.error():\n",
    "            print(f\"Received: {msg.value().decode('utf-8')}\")\n",
    "finally:\n",
    "    consumer.close()\n",
    "Run Consumer:\n",
    "\n",
    "bash\n",
    "Copy code\n",
    "python consumer.py\n",
    "Roles\n",
    "Producer: Sends messages to Kafka topics.\n",
    "Consumer: Reads messages from Kafka topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d00e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "14. Discuss the importance of data retention and data partitioning in Kafka. How can these features be\n",
    "configured, and what are the implications for data storage and processing?\n",
    "\n",
    "\n",
    "Data Retention\n",
    "Importance:\n",
    "\n",
    "Access Historical Data: Enables retrieval of past data.\n",
    "Reprocessing: Allows for reprocessing data if needed.\n",
    "Compliance: Supports regulatory and auditing requirements.\n",
    "Configuration:\n",
    "\n",
    "Retention Period: Use retention.ms to set how long messages are kept.\n",
    "Retention Size: Use retention.bytes to limit total data size.\n",
    "Implications:\n",
    "\n",
    "Storage Costs: Longer retention or more data increases storage needs.\n",
    "Performance: More data can slow down performance and recovery.\n",
    "Data Partitioning\n",
    "Importance:\n",
    "\n",
    "Scalability: Distributes data across brokers to handle more load.\n",
    "Parallelism: Allows simultaneous read/write operations.\n",
    "Fault Tolerance: Data is replicated for reliability.\n",
    "Configuration:\n",
    "\n",
    "Number of Partitions: Set with num.partitions to determine data splits.\n",
    "Replication Factor: Set with replication.factor for redundancy.\n",
    "Implications:\n",
    "\n",
    "Performance: More partitions improve performance but add management complexity.\n",
    "Data Distribution: Effective partitioning ensures balanced load across brokers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "15. Give examples of real-world use cases where Apache Kafka is employed. Discuss why Kafka is the\n",
    "preferred choice in those scenarios, and what benefits it brings to the table.\n",
    "\n",
    "Apache Kafka is a distributed streaming platform that excels in handling real-time data streams. Example of real-world use case where Kafka is employed:\n",
    "\n",
    "Real-Time Analytics\n",
    "Use Case: Companies like LinkedIn and Uber use Kafka to stream and analyze real-time data for analytics and monitoring. For instance, LinkedIn processes millions of events per second to generate real-time metrics and insights.\n",
    "Why Kafka: Kafka’s high throughput, low latency, and fault-tolerant architecture make it ideal for processing large volumes of real-time data quickly.\n",
    "Benefits: Provides real-time insights, supports high-speed data processing, and scales efficiently to handle growing data volumes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
